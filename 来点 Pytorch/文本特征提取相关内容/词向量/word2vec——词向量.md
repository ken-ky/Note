### 文本特征提取——词向量（`word2vec`）

+ **构造过程**：
  1. 词向量使用滑动窗口去记录一段文本连续出现的几个单词，这些单词构成了所谓的“窗口”
  2. 在每一窗口中，位于窗口中间的词为“目标词”（`target word`），其它词统称作“上下文”（`context words`）

+ **两类模型**：

  ![](G:\工作\暑期计划\来点pytorch\初识NLP\note something\文本特征提取相关内容\词向量\1.jpg)

  + `CBOW`(`Continuous bag-of-words`)

    通过“上下文”对“目标词”进行预测

    + `One example`
      1. 将输入上下文词独热编码得到`one-hot`向量
      2. 在词向量矩阵中找到相应的词向量（下图中`never`和`late`相应的词向量【具体词向量个数取决于窗口值大小】）
      3. 对上述的词向量进行`avg`得到新的**词向量**
      4. 再经过矩阵变化和激活（相当于神经网络）可测得`too`的概率为`0.4`
      5. 【下图未提到】：在进行`softmax`之后使用损失函数`cross entropy loss`整体进行损失统计分析，从而达到训练的效果

    ![](G:\工作\暑期计划\来点pytorch\初识NLP\note something\文本特征提取相关内容\词向量\2.jpg)

  + `Continuous skip-gram`

    通过“目标词”对“上下文”进行预测

    **不过**相较于上述方法，我们不能让神经网络一次得到**多个词**的输出，因此将其分解为多个“目标词——上下文”的形式会更加可行

    ![](G:\工作\暑期计划\来点pytorch\初识NLP\note something\文本特征提取相关内容\词向量\3.jpg)

+ 但是在上述模型之中，我们还需要考虑到当**词表足够大**时，我们的词向量维度是非常大的。因此我们需要去优化整个计算过程：

  1. 负采样（`Negative sampling`）:

     **思路**：减少需要`softmax`的词表大小（不将整个词表作为`softmax`的范围）

     **实际过程**：当$word_{predict}\ne word_{reality}$时，**不一定**要将此情况纳入**负例**，只是采样**一小部分**作为负例

     **采样标准**：根据词频进行采样，词频越高，越可能被采样
     $$
     P(w_i)=\frac{f(w_i)^{3/4}}{\sum^V_{j=1}f(w_j)^{3/4}}
     $$
     ​		其中，$f(w_i)$是$w_i$出现的频率。`3/4`是经验的放缩，防止低频词始终不被采样。此公式相当于变相进行了概率的归一化

  2. 多层`softmax`（``Hierarchial softmax`）

+ 其它相关技巧：

  + **`Sub-Sampling`**：平衡常见词和罕见词，去掉某些十分高频次的词，像是“的”之类的
    $$
    P=1-\sqrt{t/f(w)}
    $$
    ​		其中`t`是一个进行调整的参数

  + **非固定的滑动窗口**（`Soft sliding windows`）：非固定大小的窗口

    ​	设计了一种随机采样的方法：设置$S_{max}$其尺寸在`1`和$S_{max}$间随机调取，因此离`target`更近的词有更大的可能作为`context`来被训练