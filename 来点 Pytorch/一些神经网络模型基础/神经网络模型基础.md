### 神经网络模型基础笔记

+ **循环神经网络**（`RNN: Recurrent Neural Networks`）：

  ![](G:\工作\暑期计划\来点pytorch\初识NLP\note something\相关神经网络模型基础\1.jpg)

  ​	上图中的`hidden states`分作了$h_0,h_1,h_2,...,h_n$为逻辑时间排列顺序上的状态变量，其充当了记录过去及当前信息的载体（或者是变量）。

  + `RNN Cell`

    ![](G:\工作\暑期计划\来点pytorch\初识NLP\note something\相关神经网络模型基础\2.jpg)
    $$
    h_i=\tanh(W_xx_i+W_hh_{i-1}+b)\\
    y_i=F(h_i)
    $$
    ​	其中，$W_x$为针对$x_i$计算的矩阵系数，同样地$W_h$也是针对上一状态变量$h_{i-1}$进行计算的矩阵系数，而在$h_i$基础上再进行激活处理（即$F(h_i)$函数）

  + `One example`：

    ![](G:\工作\暑期计划\来点pytorch\初识NLP\note something\相关神经网络模型基础\4.jpg)

  + 应用：

    `Sequence labeling, Sequence Prediction, Photograph Description, Text Classfication`

  + 优势：

    1. 可以处理任何长度的输入
    2. 模型大小不会因为输入变长而增大
    3. 所有时间节点的权重是共享的
    4. 在计算每一步`i`时理论上是可以利用之前给出来的信息的

  + 缺点：

    1. 当前步骤的计算偏慢，因为需要上一节点数据
    2. 在实际训练中，很难利用之前节点的所有的信息

  + **梯度消失或爆炸**（`Gradient vanish or explosion`）
    $$
    h_i=\tanh(W_xx_i+W_hh_{i-1}+b)\\
    \triangle w_1=\frac{\partial Loss}{\partial w_2}=\frac{\partial h_n}{\partial h_{n-1}}\frac{\partial h_{n-1}}{\partial h_{n-1}}...\frac{\partial h_3}{\partial h_2}\frac{\partial h_2}{\partial w_2}
    $$
    对于$\frac{\partial h_n}{\partial h_{n-1}}$来说：

    + `Explosion[突然想起Megumin]`：

      `when n>1`我们会发现其式子由于链式求导法则，在反向传播过程当中其计算量会十分巨大

    + `Vanish`：

      `when n<1`这个时候我们根本无法找到$w_0$来进行计算，或者说这一梯度的变化是不存在的

  + **解决方式**【`Better Units`】

    + **消失的梯度**（`Vanishing Gradient Problem`）：

      使用更复杂的隐层单元计算当前值的模型变体：`GRU, LSTM`

      主要思路：在计算时不只依靠上一节点的数据，而且保存“附近记忆”参与计算

    + **门控循环单元**（`GRU`）：

      在传统`RNN`的基础上引入**门控机制**（即：进行筛选可以传输至下一步的信息）

      + `Vanilla RNN`：$h_i=\tanh(W_xx_i+W_hh_{i-1}+b)$
      + `Update gate`：$z_i=\sigma(W_x^{(z)}x_i+W_h^{(z)}h_{i-1}+b)$
      + `Reset gate`：$r_i=\sigma(W_x^{(r)}x_i+W_h^{(r)}h_{i-1}+b^{(r)})$
      + `New activation`$\widetilde{h_i}$（由重置门得到的）

+ **卷积神经网络**（`CNN`）：