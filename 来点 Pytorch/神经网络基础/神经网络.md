### 一些关于神经网络的基础：

+ **基本组成部分**：以前向神经网络（Feedforward Computation）为例：

  + 隐层（`Multiple hidden layyers`）：

    基本结构：每一层的输出结果是由上一层的输出结果进行线性变换激活得到

    ![](G:\工作\暑期计划\来点pytorch\初识NLP\note something\神经网路基础\1.jpg)
    $$
    \textbf{h}_1=f(\textbf{W}_1\textbf{x}+\textbf{b}_1)\\
    \textbf{h}_2=f(\textbf{W}_2\textbf{x}+\textbf{b}_2)\\
    \textbf{h}_3=f(\textbf{W}_3\textbf{x}+\textbf{b}_3)
    $$
    （`f`为激活函数）

    可是在此基础上，我们发现若激活函数是遵从线性变换的函数`f(x)`，那么上述的式子可能被线性表示为如下形式（简化`f(x)=x`）：
    $$
    \textbf{h}_1=\textbf{W}_1\textbf{x}+\textbf{b}_1,\\
    \textbf{h}_2=\textbf{W}_2\textbf{h}_1+\textbf{b}_2\\
    \\
    \textbf{h}_2=\textbf{W}_2\textbf{W}_1\textbf{x}+\textbf{W}_2\textbf{b}_1+\textbf{b}_2
    $$
    这意味着我们所进行的隐层构建是直接等价作单层的神经网络，但是我们并不希望看到这样的情况发生（我们并不希望所有抽取的特征在输出的向量上得到均等的表达效果），因此`f(x)`（即激活函数）的效果不是一个线性的反复迭代。

    因此，我们构建了一些这样的激活函数：

    1. `Sigmoid`

    $$
    f(z)=\frac{1}{1+e^{-z}}
    $$

    2. `Tanh`

    $$
    f(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}
    $$

    3. `ReLU`

    $$
    f(z)=max(z,0)
    $$

  + 输出层（`Output Layer`）：

    ![](G:\工作\暑期计划\来点pytorch\初识NLP\note something\神经网路基础\2.jpg)

    1. 线性输出（`Linear output`）：
       $$
       y=\textbf{w}^T\textbf{h}+b
       $$
       
  2. `Sigmoid`
    
     + 进行单值变换：
         $$
         y=\sigma(\textbf{w}^T\textbf{h}+b)
         $$
      
     + 二分类（`For binary classfication`）：
      
       `y for one class, 1-y for another`
      
     + 多分类（`Softmax`）：
      
       1. 进行维度变化，得到中间值`z`向量（`|z|=j`）：
            $$
            \textbf{z}=\textbf{W}\textbf{h}+\textbf{b}
            $$
      
       2. 进行归一化处理：
            $$
            y_i=softmax(z)_i=\frac{e^{z_i}}{\sum_je^{z_j}}
            $$
      
       3. 分作多类的$y_i$（`For multi-class classification`）
  
  基本代码组成：
  
  ```python
  # * 表示element-wise乘积，· 表示矩阵乘积
  
  class Layer:
      '''中间层类'''
      self.W  # (input_dim, output_dim)
      self.b  # (1, output_dim)
      self.activate(a) = sigmoid(a)/tanh(a)/ReLU(a)/Softmax(a)
  
      def forward(self, input_data):       # input_data: (1, input_dim)
         '''单个样本的前向传播'''
         input_data · self.W + self.b = a  # a: (1, output_dim)
         h = self.activate(a)              # h: (1, output_dim)
         return h
  ```
  
  

+ 补充：**反向传播**（`Back-propagation, BP`）：

  如下图所示：**前向传播通过训练数据和权重参数计算输出结果；反向传播通过导数链式法则计算损失函数对各参数的梯度，并根据梯度进行参数的更新**

  ![](G:\工作\暑期计划\来点pytorch\初识NLP\note something\神经网路基础\3.jpg)

  基本代码：

  ```python
  # * 表示element-wise乘积，· 表示矩阵乘积
  
  class Layer:
      '''中间层类'''
      self.W  # (input_dim, output_dim)
      self.b  # (1, output_dim)
      self.activate(a) = sigmoid(a)/tanh(a)/ReLU(a)/Softmax(a)
  
      def forward(self, input_data):       # input_data: (1, input_dim)
         '''单个样本的前向传播'''
         input_data · self.W + self.b = a  # a: (1, output_dim)
         h = self.activate(a)              # h: (1, output_dim)
         return h
  
      def backward(input_grad):
         '''单个样本的反向传播'''
         a_grad = input_grad * activate’(a)  # (1, output_dim)
         b_grad = a_grad                     # (1, output_dim)
         W_grad = (input_data.T) · a_grad    # (input_dim, output_dim)
  
         self.b -= learning_rate * b_grad 
         self.W -= learning_rate * W_grad
  
         return a_grad · (self.W).T          # (1, input_dim)
  ```

  

+ **模型分析**：

  + 损失函数：

    1. 均方损失（`Mean Squared Error`）：
       $$
       \min_{\theta}J(\theta)=\min_{\theta}\frac{1}{N}\sum^N_{i=1}(y_i-F_{\theta}(x_i))^2
       $$
       其中，训练样本集合为：$\{(x_i,y_i)\}^N_{i=1}$，$F_{\theta}(x_i)$为神经网络预测值

    2. 最小化交叉熵（`Cross-entropy`）：
       $$
       \min_{\theta}J(\theta)=\min_{\theta}-\frac{1}{N}\sum^N_{i=1}\log P_{model}(F_{\theta}(x_i)=y_i)
       $$
       其中，$P_{model}(F_{\theta}(x_i)=y_i)$是模型得到的预测值与标注值相等的频率（$when \thinspace i\to\infty,\thinspace f\to p$），这样的损失分析可以使用在文本分析情感中

       + 损失更新函数（`Update rule`）：
         $$
         \theta^{new}=\theta^{old}-\alpha\triangledown_{\theta}J(\theta)
         $$
         $\alpha$ `is step size or learning rate`
       
       + 梯度下降规则解释：
       
         1. 找到最快下降的方向（`find the steepest direction`）
         2. 每过固定的距离就重新寻找（`take a step`）
       
       + 那么，需要求到每一函数的梯度：
         $$
         F(x)=F(x_1,x_2,...,x_n)
         $$
       
       + 偏导的向量集合：
         $$
         \frac{\partial F}{\partial\textbf{x}}=[\frac{\partial F}{\partial x_1},\frac{\partial F}{\partial x_2},...,\frac{\partial F}{\partial x_n}]
         $$
       
       + 给定函数附有`m`个输出和`n`个输入：
         $$
         F(x)=[F_1(x_1,x_2,...,x_n),F_2(x_1,x_2,...,x_n),...,F_m(x_1,x_2,...,x_n)]
         $$
         可以使用`n`个参量代替单层神经元，那么`m`自然为神经元的层数，以下解释原因：
         $$
         \frac{\partial F_1}{\partial \textbf{x}}=[\frac{\partial F_1}{\partial x_1},\frac{\partial F_1}{\partial x_2},...,\frac{\partial F_1}{\partial x_n}]
         $$
         ​	上式为$F_1$得到的向量集合，类似于上一式的`n`变量的单层神经网络，这里便得到了另一结论：
         $$
         (\frac{\partial F}{\partial\textbf{x}})_{ij}=\frac{\partial F_i}{\partial x_j}
         $$
         有链式求导：
         $$
         \frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}
         $$
         进行带入下式：
         $$
         \textbf{h}=f(\textbf{z})\\
         \textbf{z}=\textbf{Wx}+\textbf{b}\\
         \frac{\partial\textbf{h}}{\partial\textbf{x}}=\frac{\partial \textbf{h}}{\partial\textbf{z}}\frac{\partial \textbf{z}}{\partial\textbf{x}}=...
         $$
       
       + `example`：`Given` $s=\textbf{u}^T\textbf{h},\textbf{h}=f(\textbf{z}),\textbf{z}=\textbf{Wx}+\textbf{b}$, `what is` $\frac{\partial s}{\partial\textbf{b}}$?
       
         `Apply the chain rule:`
         $$
         \frac{\partial s}{\partial\textbf{b}}=\frac{\partial s}{\partial\textbf{h}}*(\frac{\partial\textbf{h}}{\partial\textbf{z}})*(\frac{\partial\textbf{z}}{\partial\textbf{b}})\\
         \frac{\partial s}{\partial\textbf{b}}=\textbf{u}^T*diag(f'(\textbf{z}))*\textbf{1}
         $$
       
       + 而上述**链式求导**方法就是**反向传播**算法

